---
title: "Assignment 4 - Heart rate, respiration and interpersonal coordination"
author: "Riccardo Fusaroli"
date: "August 20, 2019"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Notes from class 12/11: 

Time variable: 
- it is like minutes, but each row roughly corresponds to one millisecond. 
- roughly nr pr file 200,000 - each task is roughly 3 minutes.
- nr of rows not the same for files, fine, for some P's the tasks do not take equal amount of time

# LONG FORMAT
pivot_longer() 
gather()

We want to predict MY heartrate now by my hr lagged + your hr lagged

lmer(Hr_self ~ HR_self_lag + Hr_other_lag)

# Assignment 4 - Heart rate, respiration and interpersonal coordination

Physiological data (here heart rate [variability], and respiration) are increasingly popular. Historically treated as pernicious noise to be regressed out of neuro-imaging data, there is now increasing research on how these signals tell us something important about cognition and beyond being just a signal of cognitive processes also impact them in interesting ways. Advanced sport science, and the quantified self movement (closely followed by marketing and communication) have hailed continuous physiological tracking as a powerful way to access and modify attitudes, habits, and performance. Further, as team coordination (in the military, in decision processes and organizational contexts) is more and more in focus, research has attempted to measure how interpersonal coordination between physiological systems might tell us something important about e.g. emotional and cognitive coordination. See references in the reading list for more on this.

In this assignment, you will learn to:
- pre-process physiological data (and grow further your mad R skills)
- model the continuous interdependence between two signals (using a multilevel model as proxy for a dynamical system approach)
- conservatively assess the presence of coordination between to signals in a controlled context

This assignment has two parts. The first part familiarizes you with heart rate, and respiration data and their preprocessing. The second part explores how to analyze interpersonal coordination of these signals.

These are the questions you need to be able to answer at the end of the assignment (aka that you need to submit as part of the portfolio)

1) How do you preprocess heart rate and respiration data? Describe the process. If any data needs to be excluded, list the excluded data and motivate the exclusion.

2) Do you observe interpersonal coordination in heart rate and respiration? Describe your control baseline, the method used to quantify coordination, and the statistical models used to infer whether coordination was higher than in the baseline. Report the results of the models.

3) Do you observe differences in coordination between conditions? Report the models and results.

4) Is respiration coordination a likely driver of heart rate coordination? Describe how you would test for it. Bonus points if you actually run the tests and report methods and results.

N.B. N.B. The data are collected by students from previous years (Study 1 - 4). Note that synchronous and turn-taking are the same across all four studies, but the third condition is different: in the first year it was self-paced joint reading; in the second to fourth years it was the tv-series conversation.

# notes
three different tasks, not doign the same task in the same order
we are looking into coordination, HR corrdination. We want to see whether the task itself has a role. If 2 people are in a room, maybe towards the end they are more likely to coordinate no matter the task.

Begin with: play with one file. Downsample to remove outliers. 
First one file, then write a function for the entire set.
Start with one file, not one pair. 


## Let's get started

### Exploring physiological signals
The data files can be found here: https://www.dropbox.com/sh/bvvk7t3fvsplh9o/AADM6q4WrtXKvSwH5aAO1umta?dl=0

- Choose one pair (one pair, three conditions, three files)
- Load the logs
- Produce a plot of the participants' respiration signal and a different one of the participants' HR signal.
  N.B: remember the slides: artifacts, downsampling, scaling.
  N.B. The gridExtra::grid.arrange() function allows you to display the plots side by side. E.g. grid.arrange(plot1, plot2, plot3, ncol=3). There are also smarter packages, like cowplot and ggpubr.
- Can you eye-ball which condition if any displays more physiological coordination?

### First we read one data file and identify the procedure
- Load the file
- correctly identify all columns
- plot the data
- deal with the artifacts
- downsample the data
- Add a column for study, group, trial and condition

# NEW CHUNK CLEAN
```{r}
# Load the libraries
pacman::p_load(dplyr,tidyverse,readr,ggplot2,groupdata2,cowplot, stringr,purrr,tidyr,reshape2,crqa,strex,stringr,stats)

# Load a file
#d <-  read.csv("Study1_G1_T1_Synchronous.csv")

# Downsampling 
# d1 <- d %>% group(n = 100, method = 'greedy') %>% 
#   dplyr::summarise(time = mean(time,na.rm = T),
#                    HR1 = mean(HR1, na.rm = T),
#                    HR2 = mean(HR2, na.rm = T),
#                    Resp1 = mean(Resp1, na.rm = T),
#                    Resp2 = mean(Resp2, na.rm = T)) 

# Plot before removing outliers
#p <- ggplot(data = d) + geom_path(aes(time,HR1, color = "P1")) + geom_path(aes(time,HR2,color = "P2")) + labs(x = "time", y = "HR") + theme_classic()

#p
```

# Outliers
If you remove 2 times sd from the mean, you pretty much capture 95% of the data. If you move 3 sd, you capture 99%. Up to you to decide.

We want the higher threshold to be higher than mean + threshold*sd of the timeseries. 
We want the lower threshold to be lower than mean + threshold*sd of the timeseries. 
Then, we want the function to make the extreme values be replaced by the threshold values. 
If anything is above the higher-threshold, we want it to be replaced by mean + threshold*sd of the timeseries. 
If anything is below the lower-threshold, we want it to be replaced by mean-threshold*sd of the timeseries.

```{r}

# Function for outliers
removeOuts <- function(ts, threshold){
  higher_threshold_condition <- ts > (mean(ts, na.rm = T) + (threshold*sd(ts, na.rm = T)))
  lower_threshold_condition <- ts < (mean(ts, na.rm = T) - (threshold*sd(ts, na.rm = T)))
  ts[higher_threshold_condition] <- mean(ts, na.rm= T) + (threshold*sd(ts, na.rm = T))  #anything above the higher threshold, we want to replace it
  ts[lower_threshold_condition] <- mean(ts, na.rm= T) - (threshold*sd(ts, na.rm = T))
  return(ts)
}

# defining z scale function
z_scale <- function(column){
 column_c <- (column - mean(column, na.rm=T)) / sd(column, na.rm=T)
}

# Threshold specification. 2.5 sd's from the mean
threshold = 2.5

# data preprocessing
data_preprocess <- function(filename, threshold = 2.5) {
  df <- read_delim(paste0("data/", filename), delim = ",")
   
# parse filename; study, group, trial, type
  numbers <- as.data.frame(str_extract_numbers(filename))
  Study <- numbers[1,]
  Group <- 100 * Study + numbers[2, ]
  Trial <- numbers[3,]
  Condition <- str_extract(filename, "[a-zA-Z]{9,14}")
  vars <- as.data.frame(t(cbind(c(
    Study, Group, Trial, Condition
  ))))
  names(vars) <- c("Study", "Group", "Trial", "Condition")
 
# Downsampling
  df <- df %>%
    group(n = 100, method = 'greedy') %>%
    dplyr::summarise(
      time = ifelse(Study == 4, mean(min, na.rm = T), mean(time, na.rm = T)),
      HR1 = mean(HR1, na.rm = T),
      HR2 = mean(HR2, na.rm = T),
      Resp1 = mean(Resp1, na.rm = T),
      Resp2 = mean(Resp2, na.rm = T)
    )
 
 # Remove outliers (comment out if you want to do without outliers)
  df <- df %>%
    mutate(
      HR1_c = removeOuts(HR1, threshold),
      HR2_c = removeOuts(HR2, threshold),
      Resp1_c = removeOuts(Resp1, threshold),
       Resp2_c = removeOuts(Resp2, threshold),
       HR1_c_scaled = z_scale(HR1_c),
       HR2_c_scaled = z_scale(HR2_c),
       Resp1_c_scaled = z_scale(Resp1_c),
       Resp2_c_scaled = z_scale(Resp2_c)
     )
   
  df <- cbind(vars, df)
  return(df)
}

# Identify all files to be read
# Run the function on the whole dataset using map_df
df <- list.files(path = "data/", pattern = ".csv") %>%
  purrr::map_df(data_preprocess)

df <- df %>%
  mutate(
    Study = as.factor(Study),
    Group = as.factor(Group),
    Condition = as.factor(Condition),
    Trial = as.factor(Trial),
    time = ifelse(df$time > 30000, df$time / 10000, df$time)
  ) %>%
  subset(!is.na(time)) 

df <- df %>% filter(
  Study != 1,
  Condition == 'Conversation' |
    Condition == 'Synchronous' | 
    Condition == 'TurnTaking'
)


# Plot after removing outliers
p_c <- ggplot(data = df) + geom_path(aes(time,HR1_c, color = "HR1")) + geom_path(aes(time,HR2_c,color = "HR2")) + labs(x = "time", y = "HR") + theme_classic()

p_c

# Both plots
plot_grid(p,p_c, labels = c('With outliers', 'No outliers'))
```

Outlier correction is never neutral. We can replace it with the mean, replacement value. Here, we replaced it with the threshold value to not create too much of a bias in the dataset by replacing one or the other - but there are many imperfect ways of doing it.

# Make long function + apply it to df
```{r}

make_long <- function(df) {
  
  # Mutating lead-columns
  df <-  df %>%
  group_by(Group, Condition, Trial) %>% 
  mutate(
    next_HR1 = lead(HR1_c_scaled),
    next_HR2 = lead(HR2_c_scaled),
    next_Resp1 = lead(Resp1_c_scaled),
    next_Resp2 = lead(Resp2_c_scaled),
    change_HR1 = next_HR1 - HR1_c_scaled,
    change_HR2 = next_HR2 - HR2_c_scaled,
    change_Resp1 = next_Resp1 - Resp1_c_scaled,
    change_Resp2 = next_Resp2 - Resp2_c_scaled
  ) %>%
  ungroup()
  
  # Using pivot_longer now
  long2 <-
    tidyr::pivot_longer(df, c(HR1_c_scaled, HR2_c_scaled), values_to = 'HR_self')
  
  # Mutating new columns
  long2 <- long2 %>%
    mutate(
      HR_other = tidyr::pivot_longer(df, c(HR2_c_scaled, HR1_c_scaled))[['value']],
      HR_self_lead = tidyr::pivot_longer(df, c(next_HR1, next_HR2))[['value']],
      HR_other_lead = tidyr::pivot_longer(df, c(next_HR2, next_HR1))[['value']],
      Resp_self = tidyr::pivot_longer(df, c(Resp1_c_scaled, Resp2_c_scaled))[['value']],
      Resp_other = tidyr::pivot_longer(df, c(Resp2_c_scaled, Resp1_c_scaled))[['value']],
      Resp_self_lead = tidyr::pivot_longer(df, c(next_Resp1, next_Resp2))[['value']],
      Resp_other_lead = tidyr::pivot_longer(df, c(next_Resp2, next_Resp1))[['value']],
      change_HR_self = tidyr::pivot_longer(df, c(change_HR1, change_HR2))[['value']],
      change_HR_other = tidyr::pivot_longer(df, c(change_HR2, change_HR1))[['value']],
      change_Resp_self = tidyr::pivot_longer(df, c(change_Resp1, change_Resp2))[['value']],
      change_Resp_other = tidyr::pivot_longer(df, c(change_Resp2, change_Resp1))[['value']],
      Participant = paste0(Group, str_extract_numbers(name)),
      HR_diff = HR_self - HR_other,
      Resp_diff = Resp_self - Resp_other
    )
  
  long <- long2 %>%
    select(
      -c(
        'HR1',
        'HR2',
        'Resp1',
        'Resp2',
        'HR1_c',
        'HR2_c',
        'Resp1_c',
        'Resp2_c',
        'Resp1_c_scaled',
        'Resp2_c_scaled',
        'next_HR1',
        'next_HR2',
        'next_Resp1',
        'next_Resp2',
        'change_HR1',
        'change_HR2',
        'change_Resp1',
        'change_Resp2',
        '.groups',
      )
    )
  
  return(long)
}

# Make df long
long <- make_long(df)

```

# Surrogate pairs by BUURA
Why surrogate pairs? 

You need to create list for every single study and then in the end make 1 big list. 
```{r}

View(df)
# Create lists that contain group ID's pr individual study
Groups2 <- unique(as.numeric(as.character(df$Group))[df$Study == 2])
SurrogateList2 <- expand.grid(a = Groups2, b = Groups2) %>% subset(a != b) #only those were the two groups arent the same

Groups3 <- unique(as.numeric(as.character(df$Group))[df$Study == 3])
SurrogateList3 <- expand.grid(a = Groups3, b = Groups3) %>% subset(a != b) #only those were the two groups arent the same

Groups4 <- unique(as.numeric(as.character(df$Group))[df$Study == 4])
SurrogateList4 <- expand.grid(a = Groups4, b = Groups4) %>% subset(a != b) #only those were the two groups arent the same

# In the end...
SurrogateList <- rbind(SurrogateList2,SurrogateList3,SurrogateList4)
SurrogateList

```

# Loop
```{r}
for (i in 1:nrow(SurrogateList)) {
  x <-
    subset(df, Group == SurrogateList$a[i]) #Create a subset where group = the ith element of the surrogate list a column
  y <- subset(df, Group == SurrogateList$b[i]) #Same with column b
  group <- 800 + i #Create ID's, 801, 802, so on...
  for (condition in c("Synchronous",
                      "TurnTaking",
                      "Self-paced",
                      "Conversation",
                      "MovementCoop") {
    #Making sure we pair up conditions and not only groups
    if (condition %in% unique(x$condition) &
        condition %in% unique(y$condition)) {
      z1 <- subset(x, condition == condition)
      z2 <- subset(y, condition == condition)
    }
    if (nrow(z1 > nrow(z2)) {
      z1 <- z1[-((nrow(z2) + 1):nrow(z1)), ]
    }
    if (nrow(z2 > nrow(z1)) {
      z2 <- z2[-((nrow(z1) + 1):nrow(z2)), ]
    }
    w1 <- z1 %>% mutate(
      HR2 = z2$HR2,
      Resp2 = z2$Resp2,
      HR2_lead = z2$HR2_lead,
      Resp2_lead = z2$Resp2_lead,
      HR2_change = z2$HR2_change,
      Resp2_change = z2$Resp2_change
    )
    
    w1$Group <- Group
    w1$Type <- "Surrogate"
    w <- w1
    if (exists("d_surrogate")) {
      d_surrogate <- rbind(d_surrogate, w)
      else
        (d_surrogate <- w)
    }
  }
}
```

*Function*:
?expand.grid # Creates data frame from all combinations of the supplied vectors or factors. 

# Surrogate pairs BY SÃ˜REN
```{r}
# Check which pair has lowest amount of data points
min <- df %>% group_by(Group, Condition) %>% tally()
min(min$n)

# That is 408. It looks very varying across conditions in amount of data points. We will remove that one
sp_df <- df %>% 
  filter(Group != 408)

# Check which pair has lowest amount of data points now
min <- sp_df %>% group_by(Group, Condition) %>% tally()
min(min$n)

# That is 1747. We will set it to this, so that we only have df with datapoints under 1747
sp_df <- sp_df %>% 
  filter(as.numeric(.groups) <= 1747)

# Check how many pairs
sp_df %>% group_by(.groups) %>% tally()
nlevels(sp_df$Group)

sp_df <- sp_df %>% 
  ungroup() %>% 
  mutate(Group = as.numeric(as.factor(Group)))

#nlevels(as.factor(sp_df$Group))

obj_HR <- data.frame(x = 1:5241)
obj_Resp <- data.frame(x = 1:5241)

for(i in 1:22){
  d <- sp_df %>% filter(Group == i)
  obj_HR[,i] <- d$HR2_c_scaled
  obj_Resp[,i] <- d$Resp2_c_scaled
}

list = 1:22
repeat {
  x <- as.numeric(sample(x = list, 22, replace = FALSE))
  
  if (nlevels(as.factor(x == list)) == 1 & x[1] != list[1]) {
    break
  } else
    x <- 0
}

HR2_SP <- obj_HR
Resp2_SP <- obj_Resp

for(i in 1:22){
  HR2_SP[, i] <- obj_HR[, x[i]]
  Resp2_SP[, i] <- obj_Resp[, x[i]]
}

sp_df$HR2_c_scaled <- unlist(HR2_SP, use.names = F)
sp_df$Resp2_c_scaled <- unlist(Resp2_SP, use.names = F)

long_SP <- make_long(sp_df)

```























# Scale
We want to scale the values.
```{r}
# Scaling
d1 <- d1 %>% 
  mutate(Resp1_s = scale(d1$Resp1_c),
         Resp2_s = scale(d1$Resp2_c),
         HR1_s = scale(d1$HR1_c),
         HR2_s = scale(d1$HR2_c))

### Tip: if scale() gives some issues, try the one below
#z_scale <- function(column){
#  column_c <- (column - mean(column)) / sd(column)
#}


```

## Now add the group, trial, condition to the cleaned up, scaled, downsampled data
## Tip the info is in the file name
Filename: "Study1_G1_T1_Synchronous.csv"

```{r}
filename <- "Study1_G1_T1_Synchronous.csv"

# Parse file name to extract study, group, trial, and condition
Study <- str_extract(filename, 'Study.') %>% 
  str_remove('Study')
Study

Group <- str_extract(filename, 'G.') %>% 
  str_remove('G')
Group

Trial <- str_extract(filename, 'T.') %>% 
  str_remove('T')
Trial

pos <- str_locate(filename, 'T1_')[1] #finding position of first T1_1
len_split <- length(strsplit(filename,'')[[1]]) #finding number of characters in file name
len_split 
Condition <- substr(filename,pos,len_split)
Condition <- substring(Condition,4)
Condition <- str_sub(Condition, end=-5)
Condition # NOT ELEGANT BUT WORKS

# OTHER SOLUTIONS BUT I WANT TO MOVE ON
# 
# tbl$b <- sapply(strsplit(tbl$a, " "), function(filename) filename[which.max(nchar(filename))])
# 
# 
# end <- strsplit(filename, split = "")[[1]],1)
# class(filename)
# 
# t <- word(filename,-1)
# [1] "fox"
# tail(strsplit('this is a sentence',split=" ")[[1]],1)

# Adding to dataframe
d1 <- d1 %>% 
  mutate(Study = Study,
         Group = Group,
         Trial = Trial,
         Condition = Condition, 
         )

```

```{r}
# Define a function running the loading, artifact removal, scaling, downsampling, info adding.
#df <- read.csv("Study1_G1_T1_Synchronous.csv", header = T)

data_preprocess <- function(filename, threshold = 2.5){
  # load data
  df <- read.csv(paste0("data/", filename))
  
  # downsampling
  d1 <- df %>% group(n = 100, method = 'greedy') %>% 
  dplyr::summarise(ifelse(Study == 4, time = mean(min, na.rm = T), mean(time,na.rm = T)),
                   HR1 = mean(HR1, na.rm = T),
                   HR2 = mean(HR2, na.rm = T),
                   Resp1 = mean(Resp1, na.rm = T),
                   Resp2 = mean(Resp2, na.rm = T)) 
  
  # remove outliers
  d1$HR1_c = removeOuts(d1$HR1, threshold)
  d1$HR2_c = removeOuts(d1$HR2, threshold)
  d1$Resp1_c = removeOuts(d1$Resp1, threshold)
  d1$Resp2_c = removeOuts(d1$Resp2, threshold)

  # scale
  d1 <- d1 %>% mutate(
    HR1_s = scale(d1$HR1_c),
    HR2_s = scale(d1$HR2_c),
    Resp1_s = scale(d1$Resp1_c),
    Resp2_s = scale(d1$Resp2_c))

  # info adding
  Study <- str_extract(filename, 'Study.') %>% 
  str_remove('Study')
  
  Group <- str_extract(filename, 'G.') %>% 
  str_remove('G')
  
  Trial <- str_extract(filename, 'T.') %>% 
  str_remove('T')
  
  pos <- str_locate(filename, 'T1_')[1] #finding position of first T1_1
  len_split <- length(strsplit(filename,'')[[1]]) #finding number of characters in file name
  Condition <- substr(filename,pos,len_split)
  Condition <- substring(Condition,4)
  Condition <- str_sub(Condition, end=-5)
  Condition # NOT ELEGANT PLS CHANGE
  
  # Add to Dataframe
  
  # To be filled in
  
  d1 <- d1 %>% 
  mutate(Study = Study,
         Group = Group,
         Trial = Trial,
         Condition = Condition, 
         )
  
  return(d1)

}
test <- data_preprocess("Study1_G1_T1_Synchronous.csv")

#  Identify all files to be read
#setwd("/Users/pernillebrams/Desktop/EM3/EM3_A4/data/")
getwd()
data <- list.files(path = "/Users/pernillebrams/Desktop/EM3/EM3_A4/data/", pattern = ".csv")


# Run the function on the whole data-set using map_df
hov_data <- data %>% 
  purrr::map_df(data_preprocess) #It cannot open the connection for some reason, but will open Clement's

# Now we need to make sure all the data are meaningful or something has to be removed
# E.g. "Study1_G1_T1_Synchronous" has one bad respiration signal. We could replace it with NAs

# plots plots plots

# Remove bad data

# Save the data

```

# Reading Clements data
```{r}

ECG <- read.csv("ECG.data.csv")

```


## Now we are ready to go to load and pre-process all files

Go through all the files (with a function passed onto map_df), check which files should be excluded, if any, and save the pre-processed time-series. This procedure is similar to what you have done in portfolio 3. You may use the code you wrote for that assignment and adjust it to this one.

A couple of tips:
- looping will be too slow for these files (remember you have ~200 000 rows in each file!). Making a function and using Map/Map_df is your salvation.
- you may want your first step after loading a file to be downsampling, so that you don't work with enormous amount of data
- each study restarts the group numbering, so you should make sure to change that (e.g. 100 * Study + Group)
- you need to make sure all the data are meaningful or something has to be removed. Plotting is your friend. E.g. "Study1_G1_T1_Synchronous" has one bad respiration signal. We could replace it with NAs


```{r}
# Define a function running the loading, artifact removal, scaling, downsampling, info adding.

# data_preprocess <- function(filename, threshold = 2.5){
#   
#   # To be filled in
#   
#   return(d1)
# 
# }

#  Identify all files to be read

# Run the function on the whole dataset using map_df

         
# Now we need to make sure all the data are meaningful or something has to be removed
# E.g. "Study1_G1_T1_Synchronous" has one bad respiration signal. We could replace it with NAs

# plots plots plots
# BAD PLOT from yt video: plot(ECG$HR1_c_scaled, type = "l")

# Selecting columns time and HR1
# subset <- ECG %>% 
#   select(time,HR1)
# 
# plot(subset, type = "l")
#   
# subset <- ECG %>% 
#   select(time,HR1_c_scaled)
# 
# plot(subset, type = "l") #why those weird as hell lines
  
# Remove bad data

# Save the data

```

## Now we need to run some analysis

Let's start with a multilevel model that accounts for 
- stability (how each signal is auto-correlated) (longest diagonal line, LMAX, indicator of stability)
- interpersonal dependence (each signal is dependent from the previous state of the other signal, lag)

The data needs to be further prepared, so we can analyze both participants in the same model.
We need to turn the data into a long format:
- a column indicating own hr and one own respiration
- a column indicating other hr and one other respiration
- a column indicating change in hr from previous round and one in respiration

We can then run an analysis where change is a function of one's previous state (stability, see slides), and the other's previous state (coupling). Make sure to:
- set up the most interesting contrasts: how do these parameters vary by condition? which condition should be baseline?
- set up the right random effects.
- N.B. the model will be slow. Make sure it works on a subset of the data first!

Bonus question: what if we include an additional layer? Is my heart rate just adjusting to yours, or also to how much you are adjusting to mine?
- to start answering this we can add a column indicating the previous change in hr in the other and one in respiration
- we can then build on the previous models by also adding the previous change in the other


```{r}
ECG <- read.csv("ECG.data.csv")

# Generate a column for each: previous HR1, HR2, Resp1, Resp2. Tip: use the function Lag()
# Generate a column for each: change in HR1, HR2, Resp1, Resp2
variable.names(ECG)
library(dplyr)

#Calculate lag (LEAD). Previous state / future state - we choose future state
ECG <- ECG %>% 
  group_by(Group, Condition) %>% 
  mutate(
    HR1_c_s_lead = lead(HR1_c_scaled),
    HR2_c_s_lead = lead(HR2_c_scaled),
    Resp1_c_s_lead = lead(Resp1_c_scaled),
    Resp2_c_s_lead = lead(Resp2_c_scaled))
# So in the HR1Lead, then it has moved one row up. It has the FUTURE one, the one that comes next
 
# Calculate change
ECG <- ECG %>% 
  mutate(
    change_HR1 = HR1_c_s_lead - HR1_c_scaled,
    change_HR2 = HR1_c_s_lead - HR2_c_scaled,
    change_Resp1 = Resp1_c_s_lead - Resp1_c_scaled,
    change_Resp2 = Resp2_c_s_lead - Resp2_c_scaled)
# Change is the change between HR1 before and HR1 now. 

# Make the data long, so we can analyze both participants at the same time. Tip: you can use the function gather () 
## N.B. This is a bit tricky and you might have to do it in several steps. 

# Make long
# HR
long <- tidyr::pivot_longer(ECG,c(HR1_c_scaled,HR2_c_scaled), values_to = "HR_self")

long$HR_other <- tidyr::pivot_longer(ECG,c(HR2_c_scaled,HR1_c_scaled))[['value']]

long$HR_self_lead <- tidyr::pivot_longer(ECG,c(HR1_c_s_lead,HR2_c_s_lead))[['value']]

long$HR_other_lead <- tidyr::pivot_longer(ECG,c(HR2_c_s_lead,HR1_c_s_lead))[['value']]

# Resp
long$R_self <- tidyr::pivot_longer(ECG,c(Resp1_c_scaled,Resp2_c_scaled), values_to = "Rself")[['Rself']]

long$R_other <- tidyr::pivot_longer(ECG,c(Resp2_c_scaled,Resp1_c_scaled))[['Rself']]

long$R_self_lead <- tidyr::pivot_longer(ECG,c(Resp1_c_s_lead,Resp2_c_s_lead))[['Rself']]

long$R_other_lead <- tidyr::pivot_longer(ECG,c(Resp2_c_s_lead,Resp1_c_s_lead))[['value']]

# Change
long$change_HR1_self <- tidyr::pivot_longer(ECG,c(change_HR1,change_HR2))[['value']]

long$change_HR2_other <- tidyr::pivot_longer(ECG,c(change_HR2,change_HR1))[['value']]

long$change_Resp1_self <- tidyr::pivot_longer(ECG,c(change_Resp1,change_Resp2))[['value']]

long$change_Resp2_other <- tidyr::pivot_longer(ECG,c(change_Resp2,change_Resp1))[['value']]

# Add participant
long <- long %>% 
  mutate(Participant = paste0(Group, strex::str_extract_numbers(name)))

# REMOVE NA
long <- long %>% 
  subset(!is.na(time))

# Set the most interesting contrast e.g. by defining synchronous or conversation as the baseline

levels(ECG$Condition) #"Conversation" "MovementCoop" "MovementGuided" "SelfPaced" "Synchronous" "TurnTaking"  

# Model change as a function of own and other previous state 
#change ~ own + other-previous 

# Bonus points: Add to the previous model also change in the other to see whether my adaptation is influenced by the other's adaptation.

```

Starting with a single participant:
We can
- HR Change ~ 1 + HR_self + HR_other
- HR Change ~ 1 + (HR_self + HR_other) * Condition 
- HR Change ~ 0 + (HR_self + HR_other) * Condition 

# Making models
```{r}
library(lme4)
variable.names(long)

# one p, so no random effects
HR_model1 <- lm(change_HR1 ~ 0 + (HR_self + HR_other):Condition, subset(long, Study == "3" & Participant == 3022))
summary(HR_model1)

# We can see that the coefficients from self is not good at predicting change.
# Say, the HRself:ConditionSynchronous estimate was signi., like it was in Buras, then 
# being that it is positive, it is predicting away from stability. It is predicting positive change. 

```

##Plot
```{r}
# line plot
p <- long %>% 
  subset(Study == "3" & Condition == "Conversation") %>% 
  group_by(Participant) %>% 
  mutate(time = seq(n())) %>%   # instead of having minute 1 and 2, we will create timestamps. No of rows the P has, sort of restarts the sequence
  subset(Participant == 3022) %>% 
  ggplot() + 
  geom_line(aes(time, HR_self, color = "HR_self")) + # line for self
  geom_line(aes(time, HR_other, color = "HR_other"))+ # line for other
  theme_classic()
p

# other plot with all conditions
p1 <- long %>% 
  subset(Study == "3") %>% 
  group_by(Participant, Condition) %>% 
  mutate(time = seq(n())) %>%   
  subset(Participant == 3022) %>% 
  ggplot() + 
  geom_line(aes(time, HR_self, color = "HR_self")) + # line for self
  geom_line(aes(time, HR_other, color = "HR_other"))+ # line for other
  facet_wrap(Condition~., ncol = 1) + 
  theme_classic()
p1
```

Covariation or coupling here?

The model bura made is predicting towards no change. 

Using entire dataset, subset on small scale: 
```{r}
variable.names(long)

#Model
HR_model0 <- lmerTest::lmer(change_HR1_self ~ 0 + (HR_self + HR_other):Condition + (0+Condition|Participant) + (0+Condition|Group), subset(long, Study == "3"), REML = F, control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))


# Add random effect
HR_model0 <- lmerTest::lmer(change_HR1_self ~ 0 + (HR_self + HR_other):Condition + (0+Condition|Participant) + (0+Condition|Group), subset(long, Study == "3"), REML = F, control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))

summary(HR_model0)

# Clement model
HR_model1 <- lmerTest::lmer(change_H)

```

In the above example with gather, the source columns that are gathered are specified with control:cond2. This means to use all the columns, positionally, between control and cond2. Another way of doing it is to name the columns individually.
```{r}
# # Trying to use CRQA
# ?crqa
# 
# ts1 <- ECG %>% 
#   select(time,HR1_c_scaled)
# 
# ts2 <- ECG %>% 
#   select(time,HR2_c_s_lag)
# 
# #crqa(ts1, ts2, delay, embed, rescale, radius, normalize, mindiagline, minvertline, tw, whiteline, recpt, side, method, metric, datatype)
# delay = 1; embed = 1; rescale = 0; radius = .1;
# normalize = 0; mindiagline = 2; minvertline = 2;
# tw = 0; whiteline = FALSE; recpt = FALSE; side = "both"
# method = 'crqa'; metric = 'euclidean';  
# datatype = "continuous"
# 
# ans = crqa(ts1, ts2, delay, embed, rescale, radius, normalize, 
#            mindiagline, minvertline, tw, whiteline, recpt, side, method,
#            metric, datatype)
# # Multilevel model that accounts for stability 
```

## Now we need to create control baselines.

First shuffled controls, then surrogate pairs.

### Creating controls: shuffled controls

Shuffled controls break the temporal dependencies of time-series by shuffling the value within one time-series. This ensures the "coordination" observed is not due to the actual values in the series and not their sequence.
Tip: sample() is your friend, but make sure to shuffle things within participant/condition and not throughout the whole dataset
 
```{r}
# Load new data
df <- read.csv("longV1.csv")

df$Type <- "Real"

# Create a shuffled dataset
shuff <- df %>% 
  group_by(Participant,Condition) %>% 
  mutate(
    HR_self = sample(HR_self), #randomly samples values from the column
    HR_other = sample(HR_other),
    Resp_self = sample(Resp_self),
    Resp_other = sample(Resp_other),
    change_HR_self = sample(change_HR_self),
    change_Resp_self = sample(change_Resp_self),
    Type = "Shuffled")

# Merge
dfmerge <- rbind(df,shuff)
variable.names(dfmerge)

dfmerge <- dfmerge %>% 
  mutate(diff = HR_other - HR_self)

library(lme4)
```


```{r}
# Making the diff column
df <- df %>% 
  mutate(diff = HR_other - HR_self)
df$change
# Making model - subset
diff_model <- lmerTest::lmer(change_HR_self ~ 0 + (HR_self + diff):Condition + (0+Condition|Participant), subset(df, Study == "3"), REML = F, control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))

# Making model - whole data
diff_model <- lmerTest::lmer(change_HR_self ~ 0 + (HR_self + diff):Condition + (0+Condition|Participant), df, REML = F, control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))

summary(diff_model)
View(diff_model)
# Concatenate it to the original data-set (and remember to have a column telling you which is which)

# Create the same models as in the previous chunk, but adding an interaction by shuffled vs. real

```
 
 
### TRICKY! Creating controls: surrogate pair controls
 - Per each real pair, identify at least one surrogate pair (matching one of the participants, with somebody doing the same task, but in a different pair)
 So a surrogate pair is person A from pair 1, and person A from pair 2. 

```{r}

# Identify unique pairs within a given study (to keep things manageable) and create list of possible surrogate pairs (e.g. individual 1 from pair 1 and individual 2 from pair 2)

test <- df
test$Participant <- as.character(test$Participant)
test <- test %>% 
  filter(str_locate_last(Participant) == 2)
?str_locate_last()

test <- distinct(df, Group, keep_all = TRUE)
test <- df[!duplicated(df$Group),]

unique(df$Group)

unique_pairs <- data.frame(Group,somevalue=c("x"))

unique_pairs[!duplicated(unique_pairs[,c('Group')]),]

test <- merge(unique_pairs,df)

test <- df %>% 
  filter(Condition == "TurnTaking")

test <- test %>% 
  group_by(Group) %>% 
  mutate(surrogate = ())

unique_pairs1 <- df %>% 
  group_by(Participant,Group,Condition)
surrogate_pairs <- 
  
# Starting from the wide format, create "surrogate" dataset with the data from surrogate pairs

# Make it into long format

# Create models as in chunks above, but adding an interaction with the Real vs. Surrogate variable (exclude shuffled ones for simplicity)



```
 

### Effects of respiration coordination on heart rate coordination
 - describe how you would test those.
 - Optional: run the models and report them

 